{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> A Gentle Introduction to Machine Learning: </center></h1>\n",
    "<h1><center> Basics of Linear Regression</center></h1>\n",
    "\n",
    "<div style=\"text-align: left\"> **Speaker:** *Elie Kawerk* </div>\n",
    "\n",
    "*Lebanon Artificial Intelligence Meetup*\n",
    " \n",
    "<div style=\"text-align: left\"> *$27^{th}$ of April 2017* </div>\n",
    "\n",
    "## Hands on session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this hands-on session we will implement some of the algorithms that were presented during the talk. Let us start by importing the libraries of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Plotting in notebook\n",
    "import numpy as np # Linear algebra\n",
    "import seaborn as sns # Styling our plots\n",
    "import matplotlib.pyplot as plt # Plotting interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustration purposes, we will be dealing with a toy dataset defined as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set the seed to obtain reproducible results\n",
    "np.random.seed(7)\n",
    "x = np.arange(0,10,0.2)\n",
    "y = 5.5 * x  + 1 + 2* np.random.randn(x.shape[0])\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Implementing the hypothesis\n",
    "\n",
    "The hypothesis of simple linear regression (one-dimensional feature vector) is given by: \n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta_0 + \\theta_1 x =  \\theta_0 \\underbrace{x_0}_{= 1} + \\theta_1 \\underbrace{x_1}_{= x} $$\n",
    "\n",
    "By setting **$\\theta=$** $ \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix}$ and  $X = \\begin{bmatrix}x_0 \\\\ x_1 \\end{bmatrix} $ with $x_0 = 1$ (bias) and $x_1 = x$, the hypothesis can then be expressed in the following form:\n",
    "\n",
    "$$ h_{\\theta}(x) = \\theta^{T}X  $$\n",
    "\n",
    "\n",
    "This is expression is correct for one observation. However, if we have a feature matrix of N rows in which each row presents an observation, the feature vector $X$ is written as follows:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)} \\\\\n",
    "x_0^{(2)} & x_1^{(2)} \\\\\n",
    "\\vdots & \\vdots\\\\\n",
    "x_0^{(N)} & x_1^{(N)} \n",
    "\\end{bmatrix}\\hspace{3cm}\n",
    "y = \\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "y^{(N)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then, the predicted value for each observation can be written as: \n",
    "\n",
    "$$\\hat y^{(i)} = h_{\\theta}(x^{(i)}) = \\theta^T x^{(i)} $$\n",
    "\n",
    "Or, using the matrix form, we can write:\n",
    "\n",
    "$$\\underbrace{\\hat y}_{\\begin{bmatrix}\\hat y^{(1)}\\\\ \\vdots \\\\ \\hat y^{(N)} \\end{bmatrix}} =h_{\\theta}(X) = X \\theta$$\n",
    "\n",
    "Simple matrix arithmetic tells us that if $X \\in \\mathbb{R}^{(N, D+1)}$ and $\\theta \\in \\mathbb{R}^{(D+1,1)} $ then we must obtain $\\hat y \\in \\mathbb{R}^{(N,1)}$. \n",
    "\n",
    "Let's begin by adding a column of ones (bias unit) to the feature vector x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bias(X):\n",
    "    # transform the row vector into a column vector\n",
    "    X = X.reshape(-1,1)\n",
    "    # add a column of ones\n",
    "    ones = np.ones((X.shape[0],1))\n",
    "    return np.hstack([ones,X])\n",
    "\n",
    "X = add_bias(x)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a python function called *hypothesis* that takes the vectors X and $\\theta$ as parameters (inputs) and returns an array of the predicted values $\\hat y $. You can use either a for loop or a vectorized numpy operation. For your convenience, the function's template is provided below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hypothesis(X, theta):\n",
    "    # TO DO: compute the hypothesis\n",
    "    pass\n",
    "\n",
    "theta = np.array([2.0,2.0])\n",
    "y_hat = hypothesis(X, theta)\n",
    "print(len(y), len(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for linear regression is the mean squared error defined as follows:\n",
    "\n",
    "$$J(\\theta) = \\dfrac{1}{2N} \\sum_{i=1}^{N}{\\Big ( y^{(i)}-h_{\\theta}(x^{(i)}) \\Big ) ^2}$$\n",
    "\n",
    "We can also write $J(\\theta)$ in matrix form:\n",
    "\n",
    "$$J(\\theta) = \\dfrac{1}{2N} \\Big \\lVert y- h_{\\theta}(X) \\Big \\lVert ^2\n",
    "=\\dfrac{1}{2N} \\Big( y- h_{\\theta}(X) \\Big )^T \\Big( y- h_{\\theta}(X) \\Big ) $$\n",
    "\n",
    "**Write a python function called *compute_cost* that takes X, $\\theta$ and y as parameters (inputs) and returns the cost function. You can use either a for loop or a vectorized numpy operation. For your convenience, the function's template is provided below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X,y, theta):\n",
    "    #TO DO: compute cost\n",
    "    pass\n",
    "\n",
    "print(compute_cost(X,y,theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Computing the Cost Derivative\n",
    "\n",
    "Let's compute the derivative of the cost function. For a regression model in $D$ dimensions (here $D = 1$ for simple linear regression), the derivative is a $D + 1$ dimensional vector:\n",
    "\n",
    "$$\\dfrac{\\partial J(\\theta)}{\\partial \\theta} = \\begin{bmatrix} \\dfrac{\\partial J(\\theta)}{\\partial \\theta_0} \\\\ \\vdots \\\\ \\dfrac{\\partial J(\\theta)}{\\partial \\theta_D} \\end{bmatrix}$$\n",
    "with:\n",
    "$$\\dfrac{\\partial J(\\theta)}{\\partial \\theta_k}= \\dfrac{\\partial \\Big ( \\dfrac{1}{2N} \\sum_{i=1}^{N}{\\Big ( y^{(i)}-h_{\\theta}(x^{(i)}) \\Big ) ^2} \\Big )}{\\partial \\theta_k}= \\dfrac{2}{2N} \\sum_{i=1}^{N}{\\Big ( y^{(i)}-h_{\\theta}(x^{(i)}) \\Big )\\Big(- \\dfrac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_k} \\Big)}$$\n",
    "\n",
    "Using $\\dfrac{\\partial h_{\\theta}(x^{(i)})}{\\partial \\theta_k} = x_k^{(i)}$, we get:\n",
    "\n",
    "$$\\dfrac{\\partial J(\\theta)}{\\partial \\theta_k}= - \\dfrac{1}{N} \\sum_{i=1}^{N}{\\Big ( y^{(i)}-h_{\\theta}(x^{(i)}) \\Big ) x_k^{(i)} } \\hspace{2cm} k= 0, \\dots, D$$\n",
    "\n",
    "In matrix form, the derivative ($D+1$ dimensional vector) can be written as follows:\n",
    "\n",
    "$$\\dfrac{\\partial J(\\theta)}{\\partial \\theta} = - \\dfrac{1}{N}\\hspace{2mm}X^T\\Big( y - h_{\\theta}(X) \\Big)$$\n",
    "\n",
    "\n",
    "**Write a python function called *compute_cost_derivative* that takes X, $\\theta$ and y as parameters (inputs) and returns the derivative of the cost function. You can use either a for loop or a vectorized numpy operation. For your convenience, the function's template is provided below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_derivative(X,y, theta):\n",
    "    #TO DO: compute cost derivative\n",
    "    pass\n",
    "    \n",
    "compute_cost_derivative(X,y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently the derivative of the cost function can be numerically approximated by central difference:\n",
    "\n",
    "\n",
    "$$\\dfrac{\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\dfrac{J(\\theta_k + \\epsilon)- J(\\theta_k - \\epsilon)}{2 \\epsilon} $$\n",
    "\n",
    "with $\\epsilon$ being a very small number.\n",
    "\n",
    "Let's examine if the function we have written outputs close values to ones obtained using numerical differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_cost_derivative(X,y, theta, eps=10**(-6)):\n",
    "    deriv = np.zeros(theta.shape)\n",
    "    for j in range(len(deriv)):\n",
    "        eps_vec = np.zeros(theta.shape)\n",
    "        eps_vec[j] = eps\n",
    "        deriv[j] =\\\n",
    "        (compute_cost(X,y, theta + eps_vec)- compute_cost(X,y, theta - eps_vec))/(2*eps)\n",
    "    return deriv\n",
    "\n",
    "numerical_derivative = numerical_cost_derivative(X,y, theta)\n",
    "analytical_derivative = compute_cost_derivative(X,y, theta)\n",
    "print(\"Numerical derivative:\", numerical_derivative)\n",
    "print(\"Analytical derivative:\", analytical_derivative)\n",
    "is_derivative_correct = np.abs(numerical_derivative - analytical_derivative) < 10**(-4)\n",
    "if np.all(is_derivative_correct):\n",
    "    print(\"Numerical and analytical derivatives match\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that numerical differentiation is computationally expensive and is done here only for debugging purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. Minimization with gradient descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function by descending along the tangeant to the cost function repeatedly until finding the local minimum. We first pick a value of $\\theta$ and then update this value for a certain number of epochs until convergence using the following equation:\n",
    "\n",
    "$$\\theta_j :=   \\theta_j - \\underbrace{\\alpha}_{\\text{learning rate}}\\dfrac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "We can also write the equation in vector form:\n",
    "\n",
    "$$ \\theta := \\theta - \\alpha\\dfrac{\\partial J(\\theta)}{\\partial \\theta}$$\n",
    "\n",
    "Care should be taken when chossing the learning rate $\\alpha$. A small value of $\\alpha$ would lead to a slow convergence. A great value of $\\alpha$ may lead to overshooting the minimum; the gradient descent algorithm would then have a hard time converging.\n",
    "\n",
    "**Write a python function called *update_theta* that takes X, y, $\\theta$ and alpha as parameters (inputs) and returns the updated value of theta. You can use either a for loop or a vectorized numpy operation. For your convenience, the function's template is provided below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_theta(X,y,theta,alpha):\n",
    "       # TO DO: update the value of theta using the gradient descent algorithm\n",
    "        pass\n",
    "    \n",
    "update_theta(X,y,theta, alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tying everything together\n",
    "\n",
    "Let's tie everything together. \n",
    "\n",
    "**Write a python function called *optimize_cost* that takes X, y, $\\alpha$ (the learning rate) and the number of epochs *n_epochs* as parameters (inputs) and returns the optimal $\\theta$ value. For your convenience, the function's template is provided below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_cost(X, y, alpha=0.05, n_epochs=400):\n",
    "    # TO DO: optimize cost function to find theta_optimal\n",
    "    pass\n",
    "\n",
    "theta_optimal = optimize_cost(X,y)\n",
    "y_pred = hypothesis(X,theta_optimal)\n",
    "final_cost = compute_cost(X,y, theta_optimal)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_pred, color='red', label = r\"Final cost is: \" + \n",
    "         str(round(final_cost,2)) + '\\n \\n' \n",
    "         + \"Intercept: \" + str(round(theta_optimal[0],2))+ \n",
    "         \"\\n \\n Slope: \" + str(round(theta_optimal[1],2)))\n",
    "plt.legend(frameon=True, fontsize='18', bbox_to_anchor=(1.05,1))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Understanding the behavior of the Cost function in gradient descent\n",
    "\n",
    "Plot the cost function as a function of the number of iterations using the same algorithm *optimize_cost* but instead of returning $\\theta_{optimal}$, return the vector of costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cost(X, y, alpha=0.05, n_epochs=400):\n",
    "    # TO DO: compute cost for each epoch and return a list containing all costs\n",
    "    pass\n",
    "   \n",
    "\n",
    "all_costs = get_cost(X,y)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(all_costs, marker='o')\n",
    "plt.xlim(0,30)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(r'J($\\theta$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function must decrease with time otherwise your implementation is wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Using the Scikit-learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's use the scikit-learn API to perform Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#reshape x to feed it into sklearn\n",
    "x_skl = x.reshape(-1,1)\n",
    "# instantiate the linear regression model\n",
    "lr = LinearRegression()\n",
    "# fit the model\n",
    "lr.fit(x_skl,y)\n",
    "# obtain the predicted values\n",
    "y_pred_sklearn = lr.predict(x_skl)\n",
    "#compute cost\n",
    "skl_cost = np.mean(0.5*(y-y_pred_sklearn)**2)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,y_pred_sklearn, color='red',label = \"Final cost is: \" +\n",
    "         str(round(skl_cost,2)) + '\\n \\n' \n",
    "         + \"Intercept: \" + str(round(lr.intercept_,2))+ \n",
    "         \"\\n \\n Slope: \" + str(round(lr.coef_[0],2)))\n",
    "plt.legend(frameon=True, fontsize='18', bbox_to_anchor=(1.05,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Where to go next\n",
    "\n",
    "Congratulations, you made it! Take a moment to reflect on what you learned. Here's a list of resources (that I personally found useful) to learn a lot of machine learning's practical aspects.\n",
    "\n",
    "<ul>\n",
    "<li> Machine Learning MOOC by Andrew Ng, Stanford university, Coursera (https://www.coursera.org/learn/machine-learning)\n",
    "<li> Machine Learning Mastery Blog (http://machinelearningmastery.com/)\n",
    "<li> Python Machine Learning by Sebastian Raschka, Packt Publishing (https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)\n",
    "<li> Python for Data Science and Machine Learning Bootcamp by Jose Portilla, Udemy (https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp/)\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you so much for attending this meetup!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
